## 基础

### 为什么要用

数组在申明后，不可改变大小，并且在使用过程中不够灵活，需要自定义许多操作；而使用集合，则可以在屏蔽底层实现的时候更加灵活的对数据进行处理，实现不同的数据表示关系

### fail-fast和fail-safe

这个依赖于容器内部的实现的`modCount`这个变量实现的，在增、删操作的时候，会自增这个数值。然后使用迭代器的时候，在每次`hasNext`和`next`操作时候，都会对这个数进行检查，当出现变化的就抛出`ConcurrentModificationException`，这个被称为`fail-fast`。

所以为了保证不出现错误，将原有集合进行拷贝，然后在拷贝后的数据上进行操作的方式被称为`fail-safe`

## Iterator

提供一种方法访问一个容器对象中各个元素，而又不需要对外暴露该对象内部的细节。在当前元素遍历的时候被修改，就会抛出`ConcurrentModificationException`

## ArrayList

底层使用`Object[]`实现，操作受到底层数组实现的限制，在指定位置插入和删除元素的操作的时候会大量消耗性能，在末尾插入唯一消耗性能的是触发扩容，所以最好能在使用前预估好大小。

因为是基于数组实现的，其更符合计算机底层，其在内存上是连续排布，所以在获得内存地址加偏移后能很快的获取指定元素，所以支持高效的随机元素访问。

### RandomAccess

这个接口的主要作用就是标识实现此接口的类具有随机访问能力（也表明其更贴近计算机的底层实现），当容器类实现此接口的时候，`fori`循环的速度会快于`iterator`的遍历，因为其相当于直接在内存上递增查找。

例如使用`Collections.binarySearch()` 方法时候，容器类本身实现`RandomAccess`，则会走`indexedBinarySearch()`获取更高的性能，否则则会走`iteratorSearch()`

### 扩容

默认的元素大小是10，在不指定大小的情况下调用重载的构造器构造默认大小为10的空数组`DEFAULTCAPACITY_EMPTY_ELEMENTDATA`，在指定大小为0的时候直接使用用于共享的空数组实例`EMPTY_ELEMENTDATA`，所以在默认情况下，只有在插入数据的时候才会构建，不然都是使用方法本身提供的共享空数组实例。从`Collection`对象中构建，先使用`toArray`方法直接转换，如果容量为0则用`EMPATY_ELEMENTDATA`替换原实例，否则在不是`ArrayList`对象则用`Arrays.copyOf()`方法转换后替代并保持原顺序。

**1.8的实现**

```java
     /**
     * Appends the specified element to the end of this list.
     *
     * @param e element to be appended to this list
     * @return <tt>true</tt> (as specified by {@link Collection#add})
     */
    public boolean add(E e) {
        ensureCapacityInternal(size + 1);  // Increments modCount!!
        elementData[size++] = e;
        return true;
    }

		private void ensureCapacityInternal(int minCapacity) {
        ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));
    }

		private static int calculateCapacity(Object[] elementData, int minCapacity) {
        if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) {
            return Math.max(DEFAULT_CAPACITY, minCapacity);
        }
        return minCapacity;
    }

		private void ensureExplicitCapacity(int minCapacity) {
        modCount++;

        // overflow-conscious code
        if (minCapacity - elementData.length > 0)
            grow(minCapacity);
    }
		
    /**
     * Increases the capacity to ensure that it can hold at least the
     * number of elements specified by the minimum capacity argument.
     *
     * @param minCapacity the desired minimum capacity
     */
    private void grow(int minCapacity) {
        // overflow-conscious code
        int oldCapacity = elementData.length;
        int newCapacity = oldCapacity + (oldCapacity >> 1);
        if (newCapacity - minCapacity < 0)
            newCapacity = minCapacity;
        if (newCapacity - MAX_ARRAY_SIZE > 0)
            newCapacity = hugeCapacity(minCapacity);
        // minCapacity is usually close to size, so this is a win:
        elementData = Arrays.copyOf(elementData, newCapacity);
    }

		private static int hugeCapacity(int minCapacity) {
        if (minCapacity < 0) // overflow
            throw new OutOfMemoryError();
        return (minCapacity > MAX_ARRAY_SIZE) ?
            Integer.MAX_VALUE :
            MAX_ARRAY_SIZE;
    }
```

默认直接空构造`ArrayList`，首先调用`ensureCapacityInternal`进行容量大小的判断；当添加第一个元素的时候，`minCapacity`为1，`DEFAULT_CAPACITY`为10，最后此方法会返回10；调用`ensureExplicitCapacity`方法，`minCapacity - elementData.legth > 10`成立，调用`grow`方法，第二个元素的时候这两个值相同，不会调用`grow`方法；grow的方法直接将容量进行右移一位的操作（右移n位相当于$原数据\times2^n$），此时新的容量为此前的1.5倍左右，小于最小值的时候直接用最小值替代扩容值（这个地方一般出现在指定初始化容量小于`DEFAULT_CAPACITY`或者是空构造添加第一个元素的时候）

`hugeCapacity`对于溢出的限制，当前`minCapacity`还是比数组最大值小的时候为数组最大值，否则为整数最大值；当扩容到`INTEGER.MAX_VALUE`的情况下，在下一次增加元素的情况下，`minCapacity`已经溢出，这个时候余下两个判段都会进入，最后抛出`OutOfMemoryError`错误。

`Arrays.copyOf`的内部调用`System.arrayCopy`进行实现，`copyOf`的目的是拷贝原数组指定容量的数据，并返回新数组；而`System.arrayCopy`是一个`native`方法，主要是拷贝指定数组的指定数据到目标数组，所以`copyof`内部用此来实现高效的数组拷贝。

`modCount`的作用：这是来源于`AbstractList`的一个变量，用来表示当前列表被操作的次数，使用这个数据可以进行快速的判错，入`iterator`用此来判断是否被修改而抛出异常。

`ensureCapacity`这个方法是提供给使用者使用的，如果在初始化的时候没有创建合适的容量，可以在向其添加数据前调用此方法用以扩容到合适的容量，以减少增加过程中的扩容消耗，其也是调用`ensureExplicitCapacity`来实现扩容

**1.11实现**

```java
    /**
     * Appends the specified element to the end of this list.
     *
     * @param e element to be appended to this list
     * @return {@code true} (as specified by {@link Collection#add})
     */
    public boolean add(E e) {
        modCount++;
        add(e, elementData, size);
        return true;
    }

		private void add(E e, Object[] elementData, int s) {
        if (s == elementData.length)
            elementData = grow();
        elementData[s] = e;
        size = s + 1;
    }

```

核心的扩容原理没有改变，只是前置条件改了，当插入位置已经和原有列表长度一致的时候就扩容，减少了判断次数，删除了`ensureExplicitCapacity`和`ensureCapacityInternal`方法。

### 删除操作

包含两种删除操作，下标的操作直接用`System.arraycopy`进行一个本身到本身的拷贝，相当于将目标下标后的元素进行整体移动，时间复杂度为$O(n-i)$;而元素移除的操作需要先遍历查找元素的下标，然后调用`fastremove`使用下标进行移除，这个地方也要进行下标后的数据前移

## LinkedList

1.6之前是循环列表，1.7取消了循环。

采用链表实现使其在插入数据和删除数据时候的性能都是最优的，但在指定位置删除和插入数据是最消耗性能的，时间复杂度为$O(n)$；在实际使用中，如果需要对数据进行遍历，并在遍历过程中进行删除或插入操作，这个时候最好使用此方法。

空间消耗高于`ArrayList`，要保存首位指针以及记录前驱和后继。

由于双端链表的实现外加实现了`Deque`接口的原因，所以`LinkedList`可以被当作队列进行使用。

## HashTable

内部方法通过synchronized修饰，性能较为低下。

在此类的注释上（1.8版本），如果不需要线程同步，则使用`HashMap`，需要线程同步，则使用`ConcurrentHashMap`

```java
    /**
     * Constructs a new, empty hashtable with a default initial capacity (11)
     * and load factor (0.75).
     */
    public Hashtable() {
        this(11, 0.75f);
    }
```

默认在空构造的时候默认初始容量是11，负载因子是0.75，在初始化的时候会计算出当前容量下需要`rehash`的值`threshold`。

在插入元素的时候，直接调用对象本身的`hashcode`方法获取哈希值，并与`0x7FFFFFFF`（由8421码可以确定起开头的4个比特为0111，是最小的负数）进行与操作，使其获得一个正数（哈希值不一定是正数），遍历查找是否在集合种已经存在，如果存储元素的数量超过调整容量，则会进行`rehash`操作。

`rehash`中，是直接变为`(oldCapacity << 1) + 1`个，此处对于容量最大的值的控制是$2n+1$，如果左移操作超过`MAX_ARRAY_SIZE`，新的容量还是`MAX_ARRAY_SIZE`，以此来防止溢出，构建新的结构，遍历拷贝存储。

## HashMap

1.8之前是数组+链表构成，1.8改成了当阈值为8的时候，将链表转换为红黑树（当链表长度小于64的时候是会进行扩容而不是直接转红黑树），在键值上运行使用null作为键值，

### 1.7版本

创建的默认值是16，如果指定大小，则从1开始向左移位移循环，直到找到一个$2^n$数大于或等于初始化容量大小作为构建的容量。

对于哈希函数，要求传入的是对象的`hashcode()`方法的值，对其进行一个补充的哈希计算，防止出现劣质的哈希值（也就是冲突较大，哈希后的数值分布不均匀），在注释中可以看到hashmap用的2的幂次方的哈希表，容易在地位出现冲突，`null`的作为key用于返回`0`哈希值。其在使用异或操作和右移操作确保每个位置仅有一定数量的冲突。

```java
    /**
     * Applies a supplemental hash function to a given hashCode, which
     * defends against poor quality hash functions.  This is critical
     * because HashMap uses power-of-two length hash tables, that
     * otherwise encounter collisions for hashCodes that do not differ
     * in lower bits. Note: Null keys always map to hash 0, thus index 0.
     */
    static int hash(int h) {
        // This function ensures that hashCodes that differ only by
        // constant multiples at each bit position have a bounded
        // number of collisions (approximately 8 at default load factor).
        h ^= (h >>> 20) ^ (h >>> 12);
        return h ^ (h >>> 7) ^ (h >>> 4);
    }

    /**
     * Returns index for hash code h.
     */
    static int indexFor(int h, int length) {
        return h & (length-1);
    }
```

因为1.7的代码中解决冲突的方式很暴力，就直接使用链地址法（也就是拉链法），再用`indexFor`找到要放置的链表位置，遍历查找是否有已经存在，最后在数组中指定位置插入数据，如果超过数组的存储阈值就直接扩大两倍然后拷贝到新数组中。

删除操作也是也是直接通过`indexFor`函数查出存在哪个bucket中，然后再其中进行遍历查找出数据进行删除。

### 1.8版本

对于使用默认无参的构造器进行构建，则在初始化容量的时候，如果超过最大值，则直接使用最大值作为容量`1<<30`，否则会使用`tableSizeFor`进行操作。`tableSizeFor`使用移位和异或操作进行实现的，相比于1.7的版本，其在获取数据的时候是常数级别的。

因为冲突经常发生在最低位，所以1.8处理的方式比较简单，直接右移16位，将高位移动到地位，这样在哈希算法寻找位置的时候冲突较低。

在添加元素的时候，当第一次添加元素的时候才会在添加位置创建链表分配空间，然后才进行数据的插入，此时被分为了三种情况：

1. 元素本身就是链表头节点，那直接进行后续的操作
2. 需要插入的节点是红黑树，那走红黑树的插入
3. 插入节点还是普通列表，会遍历该节点，查找是否存在同key的，有就跳出进行后续的操作，否则继续寻找，直到走到末尾节点，此时如果已经超过了默认的8个元素，就会调用`treeifyBin`进行数化（在此函数中会判断`table`数组大小是否超过64，只有超过64的时候才会进行当前树化，否则只会进行当前节点的扩容）

在扩容的`resize`的代码注释上写着，扩容后，要么保持在相同的索引位置，要么就要移动到新表中2次幂的位置。

```java
    /**
     * Implements Map.put and related methods.
     *
     * @param hash hash for key
     * @param key the key
     * @param value the value to put
     * @param onlyIfAbsent if true, don't change existing value
     * @param evict if false, the table is in creation mode.
     * @return previous value, or null if none
     */
    final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
        Node<K,V>[] tab; Node<K,V> p; int n, i;
        if ((tab = table) == null || (n = tab.length) == 0)
            n = (tab = resize()).length;
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
        else {
            Node<K,V> e; K k;
            if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            else {
                for (int binCount = 0; ; ++binCount) {
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            if (e != null) { // existing mapping for key
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }
        ++modCount;
        if (++size > threshold)
            resize();
        afterNodeInsertion(evict);
        return null;
    }

    /**
     * Computes key.hashCode() and spreads (XORs) higher bits of hash
     * to lower.  Because the table uses power-of-two masking, sets of
     * hashes that vary only in bits above the current mask will
     * always collide. (Among known examples are sets of Float keys
     * holding consecutive whole numbers in small tables.)  So we
     * apply a transform that spreads the impact of higher bits
     * downward. There is a tradeoff between speed, utility, and
     * quality of bit-spreading. Because many common sets of hashes
     * are already reasonably distributed (so don't benefit from
     * spreading), and because we use trees to handle large sets of
     * collisions in bins, we just XOR some shifted bits in the
     * cheapest possible way to reduce systematic lossage, as well as
     * to incorporate impact of the highest bits that would otherwise
     * never be used in index calculations because of table bounds.
     */
    static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }
```

### 1.11 的改变

tableSizeFor

### 多线程下问题

在多线程情况下，rehash过程会造成元素之间形成循环链表，当下一次获取形成循环的列表元素的适合，就会形成死循环，可以参考[疫苗：JAVA HASHMAP的死循环](https://coolshell.cn/articles/9606.html)

### 遍历删除

只有在使用迭代器的iterator.remove()操作删除数据是安全的。

在lambda中使用要删除数据得使用removeif操作，然后才能遍历剩下的数据。

stream操作删除数据是不安全，但是可以使用filter操作对数据进行屏蔽，然后再进行操作。

## TreeMap

基于`HashMap`实现，底层是红黑树，需要有序`map`的时候使用，其还实现了`NavigableMap`和`SortedMap`。需要多线程同步的使用`Collections.synchronizedSortedMap`方法替代。默认使用自然排序，也就是对象本身要实现`Comparable`接口，也可以在构建TreeMap的时候过程自定义比较器`Comparator`

与HashMap进行对比，其多了对集合中元素根据键值排序的能力以及对集合内元素的搜索能力

## ConcurrentHashMap

线程安全，在类注释中就直接表面这个是为了替代`HashTale`来实现的，并且兼容`HashTable`的操作方法。

### 1.7版本

内部维护了一个`Segment`的内部类，通过继承`ReentrantLock`来实现，为了简化一些锁操作，避免单独的构造。`Segment`维护了一个`entry`列表来保持一致性，所以可以在读的时候无锁，然后再`segment`的内部使用`hashentry`数组来维护数据。

**初始化**大体步骤如下：

1. 参数校验
2. 计算掩码，偏移量操作，默认构造掩码是15，偏移量28
3. 初始化，segment的容量是`sszie`,是通过循环左移1，直到找到一个小于`concurrentLevel`附近的数,
    1. `Segment[0]`的`HashEntry[]`大小是`cap`,通过`initialCapacity / ssize`计算得到`c`，然后移循环左移找到数据，其是又是一个小于`c`附近的2的幂次值
    2. `Segment[]`的大小是`sszie`
    3. 将s0放到ss中是通过`unsafe`类的`putOrderedObject`操作实现的

初始化`segment[0]`，默认为2，负载因子是0.75，扩容的阈值就是1.5，只有插入第二个数值的时候才会扩容。

**put操作**

```java
    /**
     * Applies a supplemental hash function to a given hashCode, which
     * defends against poor quality hash functions.  This is critical
     * because ConcurrentHashMap uses power-of-two length hash tables,
     * that otherwise encounter collisions for hashCodes that do not
     * differ in lower or upper bits.
     */
    private static int hash(int h) {
        // Spread bits to regularize both segment and index locations,
        // using variant of single-word Wang/Jenkins hash.
        h += (h <<  15) ^ 0xffffcd7d;
        h ^= (h >>> 10);
        h += (h <<   3);
        h ^= (h >>>  6);
        h += (h <<   2) + (h << 14);
        return h ^ (h >>> 16);
    }
```

在此版本中的hash算法，当然也就要传入调用`hashcode`方法后的值，也是遵循着冲突容易发生在低位，所以用移位，异或，求和操作得到一个值，最后再与自己右移16的数进行异或操作得到最终的哈希值。

对于存储在哪个segment位置的计算，在使用默认的构造情况下，相当于将hash值右移28位后再与掩码15进行与运算。

在发现插入位的segment为null的时候，调用ensureSegment操作进行初始化，对于这个初始化，流程大体如下：

1. 从0中获取初始化长度作为cap
2. 从0中获取负载因子
3. 计算扩容阈值
4. 创建一个cap容量的`HashEntry`数组

这个操作类似构造器初始化的时候创建`segment[0]`的数据，当然不同的是，在这个操作中，会利用unsafe类中的自旋操检测是否还是null，然后再使用unsafe类的cas操作。

在最后插入值的时候，首先是使用`tryLock`获取锁，未活得则调用`scanAndLockForPut`方法自旋的获取锁，超过`Runtime.getRuntime().availableProcessors() > 1 ? 64 : 1`的时候就使用`lock()`强制获取锁。

使用`(tab.length - 1) & hash`计算出插入的位置，对这个`HashEntry`链表进行遍历：

1. key存在，直接替换
2. 不存在，接着遍历
    1. 容量不够，扩容，也就是移动到了扩容位
    2. 移动到最后，也就是key不存在，也扩容结束，就使用头插法插入数据

**扩容rehash**

从注释和实际代码中，我们可以看到，扩容是直接扩容为原来的2倍，老数组里的数据移动到新的数组中的时候，要么不变，要么是index+oldsize，最后用头插法插入新节点。

### 1.8版本

当然和同版本的HashMap一样，修改成了链表变红黑树的实现，对于位置从原来的`segment`修改成了`node`数组。对于构造器，只是申明的大小，只有在插入数据的时候才会去开辟空间。

在操作前面有`tabAt`，`casTabAt`和`setTabAt`三个前置操作，进行数据可见性的操作，`setTabAt`操作始终锁定在锁定区域内进行，因此只要求发布顺序。

对于`hash`函数，其与`HashMap`不同，是在右移16，减少冲突后，然后在与本身进行异或操作，这个适合会出现负数请求，所以再与`0x7fffffff`进行并操作后，就能获取正数。

```java
    /**
     * Spreads (XORs) higher bits of hash to lower and also forces top
     * bit to 0. Because the table uses power-of-two masking, sets of
     * hashes that vary only in bits above the current mask will
     * always collide. (Among known examples are sets of Float keys
     * holding consecutive whole numbers in small tables.)  So we
     * apply a transform that spreads the impact of higher bits
     * downward. There is a tradeoff between speed, utility, and
     * quality of bit-spreading. Because many common sets of hashes
     * are already reasonably distributed (so don't benefit from
     * spreading), and because we use trees to handle large sets of
     * collisions in bins, we just XOR some shifted bits in the
     * cheapest possible way to reduce systematic lossage, as well as
     * to incorporate impact of the highest bits that would otherwise
     * never be used in index calculations because of table bounds.
     */
    static final int spread(int h) {
        return (h ^ (h >>> 16)) & HASH_BITS;
    }
```

**get操作**

在get操作的适合，使用`tabAt`来进行可见性操作。其原理比较简单，根据哈希值和tabat找到node节点，头简单就是就返回，是红黑树就用find函数查找，是链表就遍历查找。

**put操作**

当添加到`table`表空间点的时候：

1. 为null，也就刚申明好，这个时候使用`initTable`调用进行空间的开辟
2. 桶内为空，也就是刚初始化好，进行的第二次循环，这个时候直接使用cas操作放入数据
3. 当当前位置的`hashcode == MOVED == -1`就进行扩容，和之前的tabat操作有关
4. 代码顺序，在tabat操作的时候已经找到了需要插入node节点的位置，在下一次循环的时候就可以进行真实的数据插入操作；
    1. 直接使用synchronized锁住当前节点
    2. 再次使用tabat进行同步，fh值是通过hash得到的，为正数的时候说明还是最初的列表，然后在次节点链表中进行遍历，同时进行bincount的自增操作，key相同则替换数据，移动到末尾则插入数据
    3. 如果是红黑树，则使用红黑树的插入操作。
5. 最后检查bincount的值，大于树化阈值则进行书化操作

**remove操作**

通过浏览remove的代码，可以看到大体逻辑和put操作类似，但是可以发现，在红黑树删除的代码里面，有这么一行`setTabAt(tab, i, untreeify(t.first));`，也就是说当冲突减小到阈值的时候，又会变为链表。

## Set

`HashSet`基于`HashMap`实现，使用对象成员的`hashcode`来计算哈希值，当哈希值相同时候，会使用`equals`方法来判断两个对象是否相同

`LinkedHashSet`是`HashSet`的子类，按照添加的顺序遍历，`TreeSet`底层使用红黑树实现，能够按照添加元素的顺序进行遍历，排序方式有自然排序和定制排序。